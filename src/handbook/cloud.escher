
BuildCloudPage {
        wf *io.WriteFile
        wf:Content = *CloudPage
        wf:Name = "cloud.html"
        : = wf:Ready
}

CloudPage {
	h *Html
	h: = :
	h:Title = "Paradigm for building clouds with Circuit and Escher"
	h:Body = t:

	f *Fork
	f:Diagram = dia:

	dia *FigurePngSvg
	dia:Image = "cloud"
	dia:Width = "600px"
	dia:Caption = ``

	t *text.QuickForm
	t:Data = f:
	t:Form = `

<h1>Paradigm for building clouds with Circuit and Escher</h1>

<p>In this article our goal is to describe a framework for building, controlling and maintaining
cloud applications comprised of large numbers of interconnected services.

<h2>Framework</h3>

<p>Every well-defined system requires a clear specification of the objects
at play, their possible interrelations in any moment in time, as well as the allowable operations
that can be performed to its components.

<p>The systems of interest here, which model cloud applications in the datacenter, have
three types of components: hosts, services and links. We will treat these objects cleanly in
a formal manner, but it should be clear that they will end up corresponding to well-known
real technologies utilized in specific manners.

<p>Our hosts will correspond to physical machines (or virtual machines, as the case might be).
Our services will correspond to <a href="http://docker.com">Docker</a> containers, whose images
are configured in a standard manner to expect a number of named incoming or outgoing TCP connections.
And each of our links will correspond to a pair of lightweiht DNS servers, one at each endpoint host, 
configured to point the respective Docker TCP connections at each other.

<p>The exact details of the correspondence between hosts, services and links, and machines, Docker
containers and DNS servers, respectively, will be fleshed out in a later section. For now, suffice it to say
that this correspondence will be made simple and natural through the use of the <a href="http://gocircuit.org">gocircuit.org</a>
tool for dynamic cloud orchestration (although with an appropriate driver a similar result can be accomplished
with any cloud provider like <a href="https://cloud.google.com/compute/">Google Compute Engine</a> 
or <a href="aws.amazon.com/ec2/">Amazon EC2</a>, for instance).

<p>Getting back to the abstract system framework, the allowed relationships between hosts, services and links
are described in a few simple postulates:

<ul>
<li>Every host in the system is identified by a unique string identifier
<li>Every service “resides” on one host and every such service has a string identifier, unique only across the services residing 
on the same host.
<li>Every service has a “type” denoted by a string (which will correspond to the Docker image name of its container).
<li>Every service can have zero or more named “valves” (where a valve will correspond to a TCP connection, client or server)
under the requirement that valve names are unique within one service.
<li>Every link “connects” one service-valve pair to another, so that no such pair is connected more than once.
</ul>

<p>Relationships between the components of a system can be represented visually using the same 
<a href="syntax.html">symbolism employed by Escher for representing nested circuits</a>:

{{.Gate.Diagram}}

<p>In the illustration above there are two hosts named <code>host1</code> and <code>host2</code>.
Two services, named <code>cache</code> and <code>server</code>, reside on <code>host1</code>.
One service, named <code>database</code>, resides on <code>host2</code>. Service <code>cache</code>
is of type <code>MemCache</code>, service <code>server</code> is of type <code>Http</code> and
service <code>database</code> is of type <code>Redis</code>. There are two links in the system:
one connecting the service-valve pair <code>(server, x)</code> to <code>(cache, y)</code>, and
one connecting <code>(cache, z)</code> to <code>(database, w)</code>. (Disregard the labels
<code>p</code> and <code>q</code> for now.)

<p>Thus far we have addressed the properties describing the state of a system in a singular moment in time.
System state can change over time, or “dynamically”, according to the following additional postulates:

<ul>
<li>Hosts, services and links can “emerge” and “disappear” asynchronously from the system.
<li>When a host disappears, all services residing on it disappear as well.
<li>When a service disappears, all links incident to it disappear as well.
</ul>

<p>In particular, hosts, services and links can appear independently of each other.

<p>Some of these dynamic events (of emergence or disappearance) will be caused by external 
factors (for instance a host might die due to bad hardware) and others will be caused by operations
that we perform with the system (for instance, we might start a service). No matter what the cause
for an event is, the important thing is that these are the only changes of state that can happen to
the system.

<h2>Representation</h3>

<p>The symbolic visual representation of system state, exemplified above, can very well be used
as a formal representation, much like architectural blueprints are used as formal representations
of building design. However, this visual representation while natural for people is not easy to use
by machines.

<p>As we explain in the section on <a href="syntax.html">Escher syntax</a>, this visual representation
has an equivalent syntactic (i.e. written) form, which is well-suited for machine manipulations.
In particular, the syntactic representation of the diagram above would be as follows:

<pre>
{
	host1 {
		cache MemCache
		server Http
		server:x = cache:y
		cache:z = :p
	}
	host2 {
		database Redis
		database:w = :q
	}
	host1:p = host2:q
}
</pre>

<p>In other words, every system state can be represented in the form of an Escher circuit. This gives
us a two-fold benefit. 

<p>On the one hand, Escher circuits can be manipulated programmatically (both from Go and from Escher)
simply as data structures. This allows flexible programmatic investigation of system state through familiar technologies.

<p>On the other hand, Escher's <a href="program.html">programming and materialization mechanism</a>
allows for such circuits to be built out modularly from smaller component circuits. In other words, large
datacenter topologies can be composed out of smaller standard components, whereby even the components
circuits can span multiple machines and themselves be non-trivial subsystems.

<p>For instance, our example system state could be generated out of smaller components in the following manner.
Let the following circuit be an <a href="meaning.html">index</a> (i.e. a library), consisting of two circuits designs:

<pre>
Index {
	HttpHost {
		cache MemCache
		server Http
		server:x = cache:y
		cache:z = :p
	}
	DbHost {
		database Redis
		database:w = :q
	}
}
</pre>

<p>Then, if we <a href="program.html">materialize</a> the program

<pre>
{
	host1 HttpHost
	host2 DbHost
	host1:p = host2:q
}
</pre>

relative to <code>Index</code>, the resulting residue will be precisely the system state circuit that
we started with, i.e. the one illustrated in the drawing above.

<h2>Dual representation</h2>

<p>We call the circuit representation of system state, described thus far, a “primal” representation or simply a primal.
Every primal has an equivalent “dual” representation. Transitioning from primal to dual and vice-versa is a matter of
a simple transformation, as we shall see. 

<p>The dual representation of system state is useful to us, as it is more convenient to carry out certain manipulations
within this representation. In particular, it will be easier to compute the difference between two states in the dual.
As well as it will be easier to “materialize” a dual system state description into an actual running datacenter topology.

<p>The dual representation of a system state primal consists of two lists: a list of services and a list of links.

<p>The list of services simply enumerates all services found in the primal, each specified by its “full path”
in the primal, accompanied by its type. For our running example, the list of services would be

<pre>
(host1.cache, MemCache)
(host1.server, Http)
(host2.database, Redis)
</pre>

<p>The list of links enumerates all service-to-service links present in the primal representation as pairs 
of endpoints, wherein each endpoint (a service-valve pair) is also specified by its “full path” in the primal.
In our example, that list would be:

<pre>
(host1.server:x, host1.cache:y)
(host1.cache:z, host2.database:w)
</pre>

<p>It is not hard to see how the primal can be derived from the dual by reversing this process.

<p>Furthermore, it is self-evident that one can compute the “difference” between two systems, when
this makes sense, by simply computing the difference of their corresponding dual representations elementwise.

<h2>Sensing and materializing</h2>

<p>Sensing and materializing are the two operations that convert between the abstract circuit 
representation of a cloud topology and the actual physical topology that executes on the cloud.

<p>Sensing is the operation of “reading” the current state of the cloud and representing it in the
primal form for the engineer to work with.

<p>Materializing is the operation of “writing” (or “executing”) a cloud topology in primal form
into an actual physical network of services running in the cloud.

<p>In the following sections we describe how sensing and materializing to and from dual form
work. The subsequent conversions from dual to primal, a mere data structure transformation,
was explained in the previous section.

<p>The specific API for manipulating the cloud can be any: 
Google Compute Engine, Amazon EC2, <a href="http://gocircuit.org">Circuit</a>, and 
so forth.  Our following explanations will be based on the Circuit as its simple API provides
exactly the minimum necessary for such manipulations.

<h3>Preparing Docker service containers</h3>

<p>We have chosen to use executable Docker containers as embodiment for services.

<p>Each service communicates with the outside—with other services—through a set
of zero or more named valves. A valve corresponds to a TCP client connection, a TCP server
connection or both.

<p>Service container images must be prepared in a standardized manner so that after the
execution of a container, our framework can (i) obtain the TCP server address corresponding
to each valve (if there is one), as well as (ii) supply the remote TCP server address if the
valve also corresponds to a TCP client connection.

<p>There are various ways to prepare Docker containers to accomplish this and we do not
mandate a specific one. Here, we merely suggest one way of doing it without going into
unnecessary technical detail.

<p>To accomplish (i), one can utilize the <a href="https://docs.docker.com/userguide/dockerlinks/">Docker 
port mapping</a> mechanism. In particular, the software inside the container can be hardwired to
listen to specific port numbers which, in lexicographic order, correspond to the valve names
of the service. Once the container is executed, the effective TCP server addresses—those visible to
other containers in the cloud network—can be automatically obtained using the <code>docker port</code>
command. They will be utilized by our system to “link” containers (i.e. service valves) in a manner described later.

<p>To accomplish (ii), we propose configuring each Docker service container to use a DNS
server whose address is passed on it upon execution, using any one of the various mechanisms
available for passing arguments to containers upon execution, provided by Docker itself.
Subsequently, the software executing inside the Docker container should simply be hardwired
to obtain the IP address for any given valve name by simply looking up that valve name (perhaps
prefixed by a standard domain name) through the DNS system. Our framework, described later,
which executes the Docker containers will arrange for a light-weight dedicated DNS server
for each container, whose sole job would be to resolve these queries appropriately.

<h3>Materializing a dual form to the cloud</h3>

<p>Let us consider the task of materializing the system from our running example which,
as we showed above, has the following dual form. The list of services is:

<pre>
(host1.cache, MemCache)
(host1.server, Http)
(host2.database, Redis)
</pre>

And the list of links is:

<pre>
(host1.server:x, host1.cache:y)
(host1.cache:z, host2.database:w)
</pre>

<p>Materialization proceeds like so:

<ol>
<li>Obtain a list of available and unused hosts in the cloud.
<p>The <a href="http://gocircuit.org">Circuit API</a> 
presents all of its resources uniformly as a file system, where root level directories correspond to available hosts.
Unused hosts are precisely those root level directories that have no children (i.e. no services or other Circuit elements
are running on them). Such a list can be obtained through the API or through the command line using
<code>circuit ls /...</code>. Let us assume, for instance, that the list of available and unused hosts is
<pre>
/X65cc3c8e31817756
/Xe4abe0c286b0e0bc
/X9738a5e29e51338e
</pre>

<li>Group the elements of the list of services (from the dual) by host and assign a unique (available and unused)
Circuit host to each of the hosts from dual. For instance:
<pre>
(/X65cc3c8e31817756, host1)
(/Xe4abe0c286b0e0bc, host2)
</pre>

<li>Execute every service in the dual, as follows. Take, for instance, the service
<pre>
(host1.cache, MemCache)
</pre>

<ul>
<li>Create a dedicated light-weight DNS server for this service, on the Circuit host assigned to this service in the previous step.
Using the Circuit, we spawn a DNS element and choose its name to follow this convention:
<pre>
/X65cc3c8e31817756/host1/cache/dns
</pre>
<p>This is accomplished using the Circuit <code>circuit mkdns</code> command. The details of this are omitted for brevity.
Initially the DNS server will have no resource records, i.e. it will not resolve any lookups. Appropriate records will be added
to it later, when we materialize the list of links from the dual form.
<li>Execute the service's Docker container on that same host using a similar naming convention:
<pre>
/X65cc3c8e31817756/host1/cache/service
</pre>
<p>This is accomplished using the Circuit's <code>circuit mkdkr</code> command, and recall that the service type,
<code>MemCache</code> in this case, is used as the name of the Docker image to be used. Furthermore, the IP address of the DNS server created in the previous step is passed to the Docker container on execution.
</ul>

<li>For each link in the list of links, add DNS resource records to the appropriate DNS servers. 
Take for instance the link:
<pre>
(host1.cache:z, host2.database:w)
</pre>

<ul>
<li>??
</ul>

</ol>

<h3>Sensing the cloud state into a dual form</h3>

<p>…

<h2>Control loop</h2>

<p>…

<h2>The resulting UI to the engineer</h2>

<p>…

`
}
